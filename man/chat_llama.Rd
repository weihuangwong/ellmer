\name{chat_llama}
\alias{chat_llama}
\title{Chat with a llama-server}
\usage{
chat_llama(
  base_url,
  system_prompt = NULL,
  model = NULL,
  params = NULL,
  api_args = list(),
  api_key = NULL,
  echo = NULL
)
}
\arguments{
\item{base_url}{The base URL to the endpoint; the default uses OpenAI.}

\item{system_prompt}{A system prompt to set the behavior of the assistant.}

\item{model}{The model to use for the chat. The default, \code{NULL}, will pick
a reasonable default, and tell you about. We strongly recommend explicitly
choosing a model for all but the most casual use.}

\item{params}{Common model parameters, usually created by \code{\link[=params]{params()}}.}

\item{api_args}{Named list of arbitrary extra arguments appended to the body
of every chat API call. Combined with the body object generated by ellmer
with \code{modifyList()}.}

\item{api_key}{Optional API key if your llama-server requires authentication}

\item{echo}{One of the following options:
\itemize{
\item \code{none}: don't emit any output (default when running in a function).
\item \code{text}: echo text output as it streams in (default when running at
the console).
\item \code{all}: echo all input and output.
}

Note this only affects the \code{chat()} method.}
}
\value{
A \link{Chat} object.
}
\description{
This function connects to a llama-server instance that provides an OpenAI-compatible API.
Note that some llama-server implementations may not include role fields in their 
responses, which this function handles automatically.
}
\examples{
\dontrun{
chat <- chat_llama("http://localhost:8000")
chat$chat("Tell me three jokes about statisticians")
}
}